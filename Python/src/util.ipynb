{"cells":[{"cell_type":"markdown","source":["Holds all utility functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e555e54c-16e5-4ff4-b25e-6f5fcdd03aad"}}},{"cell_type":"code","source":["\n%run ../includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"806a320a-9e2b-46cf-8fd6-be8425b0edae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: DataFrame[]"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import requests\nimport json\nfrom delta.tables import DeltaTable\nfrom pyspark.sql import DataFrame, DataFrameWriter\nfrom pyspark.sql.functions import (\n    col,\n    current_timestamp,\n    from_json,\n    from_unixtime,\n    lit,\n    explode\n)\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.utils import AnalysisException"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b234946-1d6c-45cc-b5b4-f397726cd2b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_dataframe_writer(\n    dataframe: DataFrame,\n    partition_column: str = None,\n    mode: str = \"append\",\n    format: str = \"delta\"\n) -> DataFrameWriter:\n  \n  \"\"\"\n  Creates a dataframe writer.\n  \n  Parameters:\n    dataframe (DataFrame): spark dataframe.\n    partition_column (string): name of partition column\n    mode (string): write mode operation\n    format (string): write file format\n  \n  Returns:\n    DataFrameWriter: Spark dataframe writer object.  \n  \"\"\"\n  \n  dataframe_writer = ( \n    dataframe.write.format(format).mode(mode)\n  )\n  if partition_column is not None:\n    return dataframe_writer.partitionBy(partition_column)\n  return dataframe_writer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"714e4a5b-d71b-45d5-b66d-d52868030fab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def read_api_raw(spark: SparkSession, url: str) -> DataFrame:\n  \"\"\"\n  Read data from given REST API url.\n  \n  Parameters:\n    spark (SparkSession): Running spark session object.\n    url (string): REST API endpoint.\n  \n  Returns:\n    DataFrame: Spark dataframe having raw api data.  \n  \"\"\"\n  \n  api_data = requests.request('GET', url)\n  dbutils.fs.put(ingestionPipelinePath+'temp/api_raw_data.json', api_data.text)\n  raw_df = spark.read.option('multiline', 'true').json(ingestionPipelinePath+'temp/api_raw_data.json')\n  raw_df = raw_df.withColumnRenamed('bikes', 'raw_data')\n  \n  return raw_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"211c95dd-7f0f-488e-a7b6-bbff0927d7a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def add_metadata_raw(raw: DataFrame) -> DataFrame:\n  \"\"\"\n  Adds metadata information to RAW data.\n  \n  Parameters:\n    raw (DataFrame): spark dataframe.\n  \n  Returns:\n    DataFrame: Spark dataframe with metadata columns added.  \n  \"\"\"\n  return (\n    raw.select\n      (\n        lit(\"https://bikeindex.org/api/v3/search\").alias(\"datasource\"), \n        current_timestamp().alias(\"ingesttime\"), \n        \"raw_data\",\n        current_timestamp().cast(\"date\").alias(\"p_ingestdate\")\n      )\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"731394fc-30f9-48e4-b3db-9843aa468c86"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def transform_landing(landing: DataFrame) -> DataFrame:\n  \"\"\"\n  Apply transformations on landing data.\n  \n  Parameters:\n    landing (DataFrame): spark dataframe.\n  \n  Returns:\n    DataFrame: Spark dataframe.  \n  \"\"\"\n  return (\n      landing.select(\n        explode('raw_data').alias('data')\n      )\n      .select(col('data.*'))\n      .withColumn('p_incident_date', current_timestamp().cast(\"date\"))\n    )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd7c524b-2f9f-4f5a-9d92-bb1be58e91ee"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def add_to_metastore(table_name: str, path: str) -> None:\n  \"\"\"\n  Creates table in a metastore.\n  \n  Parameters:\n    table_name (string): name of the table to be registered in the metastore.\n    path (string): path to the underlying data files.\n  \n  Returns:\n    None\n  \"\"\"\n  \n  spark.sql(\n    \"\"\"\n      DROP TABLE IF EXISTS {}\n    \"\"\".format(table_name)\n  )\n\n  spark.sql(\n    \"\"\"\n      CREATE TABLE {}\n      USING DELTA \n      LOCATION \"{}\"\n    \"\"\".format(table_name, path)\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"126fab28-224d-49be-aede-d01b3e0fb210"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def insert_update_processed(stage: DataFrame) -> None:\n  \"\"\"\n  Apply INSERT or UPDATE operation based on the state of table. \n  i.e. if it is the very first load or insertion of new record there would be straightforward insert\n  otherwsie there will be an update operation performed\n  \n  Parameters:\n    stage (DataFrame): spark stage dataframe.\n  \n  Returns:\n    None\n  \"\"\"\n  \n  try:\n    deltaTable = DeltaTable.forPath(spark, path=processedPath)\n    deltaTable.alias('oldData').merge(stage.alias(\"newData\"), \"oldData.id = newData.id\").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n  except AnalysisException as ex:\n    (\n      stage\n      .write.format(\"delta\")\n      .mode(\"append\")\n      .partitionBy(\"p_incident_date\")\n      .save(processedPath)\n     )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20f8afc8-0f0e-4a76-a6ff-ac3f68d3613a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ec24fc3-6bcf-4bef-b628-2e479e150967"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"util","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2228298773724081}},"nbformat":4,"nbformat_minor":0}
